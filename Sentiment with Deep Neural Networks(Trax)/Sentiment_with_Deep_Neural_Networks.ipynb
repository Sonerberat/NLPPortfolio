{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment with Deep Neural Networks"
      ],
      "metadata": {
        "id": "Pe-ytE03tI8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries and try out Trax"
      ],
      "metadata": {
        "id": "ptNJPVF3t6Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import shutil\n",
        "import random as rnd\n",
        "\n",
        "# import relevant libraries\n",
        "import trax\n",
        "import trax.fastmath.numpy as np\n",
        "from trax import layers as tl\n",
        "from trax import fastmath"
      ],
      "metadata": {
        "id": "e4bLJtwmtJVc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import Layer from the utils.py file\n",
        "from utils import Layer, load_tweets, process_tweet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzEocpSjtTzU",
        "outputId": "72cc86f2-1a6d-4af6-ea04-0be5deabf84a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading in the data"
      ],
      "metadata": {
        "id": "ZSLdo1MCuOrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split():\n",
        "    # Load positive and negative tweets\n",
        "    all_positive_tweets, all_negative_tweets = load_tweets()\n",
        "\n",
        "    # View the total number of positive and negative tweets.\n",
        "    print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
        "    print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
        "\n",
        "    # Split positive set into validation and training\n",
        "    val_pos   = all_positive_tweets[4000:] # generating validation set for positive tweets\n",
        "    train_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\n",
        "\n",
        "    # Split negative set into validation and training\n",
        "    val_neg   = all_negative_tweets[4000:] # generating validation set for negative tweets\n",
        "    train_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\n",
        "    \n",
        "    # Combine training data into one set\n",
        "    train_x = train_pos + train_neg \n",
        "\n",
        "    # Combine validation data into one set\n",
        "    val_x  = val_pos + val_neg\n",
        "\n",
        "    # Set the labels for the training set (1 for positive, 0 for negative)\n",
        "    train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "\n",
        "    # Set the labels for the validation set (1 for positive, 0 for negative)\n",
        "    val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
        "\n",
        "\n",
        "    return train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y"
      ],
      "metadata": {
        "id": "vbQjPCxItk_g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y = train_val_split()\n",
        "\n",
        "print(f\"length of train_x {len(train_x)}\")\n",
        "print(f\"length of val_x {len(val_x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruoF_bJxuAVr",
        "outputId": "82d126c6-a0a7-4ee9-eef8-03ec98946ea2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of positive tweets: 5000\n",
            "The number of negative tweets: 5000\n",
            "length of train_x 8000\n",
            "length of val_x 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try out function that processes tweets\n",
        "print(\"original tweet at training position 0\")\n",
        "print(train_pos[0])\n",
        "\n",
        "print(\"Tweet at training position 0 after processing:\")\n",
        "process_tweet(train_pos[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2mn1sAjuCFq",
        "outputId": "4315f48b-2a41-4ece-b28b-d244be229752"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original tweet at training position 0\n",
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "Tweet at training position 0 after processing:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the vocabulary"
      ],
      "metadata": {
        "id": "b-F6c8FFuVd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(train_x):\n",
        "\n",
        "    # Include special tokens \n",
        "    # started with pad, end of line and unk tokens\n",
        "    Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
        "\n",
        "    # Note that we build vocab using training data\n",
        "    for tweet in train_x: \n",
        "        processed_tweet = process_tweet(tweet)\n",
        "        for word in processed_tweet:\n",
        "            if word not in Vocab: \n",
        "                Vocab[word] = len(Vocab)\n",
        "    \n",
        "    return Vocab\n",
        "\n",
        "Vocab = get_vocab(train_x)\n",
        "\n",
        "print(\"Total words in vocab are\",len(Vocab))\n",
        "display(Vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YBXTdrWZuF9u",
        "outputId": "d4707116-eb88-431a-a445-aecbf932b3e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in vocab are 9089\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'__PAD__': 0,\n",
              " '__</e>__': 1,\n",
              " '__UNK__': 2,\n",
              " 'followfriday': 3,\n",
              " 'top': 4,\n",
              " 'engag': 5,\n",
              " 'member': 6,\n",
              " 'commun': 7,\n",
              " 'week': 8,\n",
              " ':)': 9,\n",
              " 'hey': 10,\n",
              " 'jame': 11,\n",
              " 'odd': 12,\n",
              " ':/': 13,\n",
              " 'pleas': 14,\n",
              " 'call': 15,\n",
              " 'contact': 16,\n",
              " 'centr': 17,\n",
              " '02392441234': 18,\n",
              " 'abl': 19,\n",
              " 'assist': 20,\n",
              " 'mani': 21,\n",
              " 'thank': 22,\n",
              " 'listen': 23,\n",
              " 'last': 24,\n",
              " 'night': 25,\n",
              " 'bleed': 26,\n",
              " 'amaz': 27,\n",
              " 'track': 28,\n",
              " 'scotland': 29,\n",
              " 'congrat': 30,\n",
              " 'yeaaah': 31,\n",
              " 'yipppi': 32,\n",
              " 'accnt': 33,\n",
              " 'verifi': 34,\n",
              " 'rqst': 35,\n",
              " 'succeed': 36,\n",
              " 'got': 37,\n",
              " 'blue': 38,\n",
              " 'tick': 39,\n",
              " 'mark': 40,\n",
              " 'fb': 41,\n",
              " 'profil': 42,\n",
              " '15': 43,\n",
              " 'day': 44,\n",
              " 'one': 45,\n",
              " 'irresist': 46,\n",
              " 'flipkartfashionfriday': 47,\n",
              " 'like': 48,\n",
              " 'keep': 49,\n",
              " 'love': 50,\n",
              " 'custom': 51,\n",
              " 'wait': 52,\n",
              " 'long': 53,\n",
              " 'hope': 54,\n",
              " 'enjoy': 55,\n",
              " 'happi': 56,\n",
              " 'friday': 57,\n",
              " 'lwwf': 58,\n",
              " 'second': 59,\n",
              " 'thought': 60,\n",
              " '’': 61,\n",
              " 'enough': 62,\n",
              " 'time': 63,\n",
              " 'dd': 64,\n",
              " 'new': 65,\n",
              " 'short': 66,\n",
              " 'enter': 67,\n",
              " 'system': 68,\n",
              " 'sheep': 69,\n",
              " 'must': 70,\n",
              " 'buy': 71,\n",
              " 'jgh': 72,\n",
              " 'go': 73,\n",
              " 'bayan': 74,\n",
              " ':d': 75,\n",
              " 'bye': 76,\n",
              " 'act': 77,\n",
              " 'mischiev': 78,\n",
              " 'etl': 79,\n",
              " 'layer': 80,\n",
              " 'in-hous': 81,\n",
              " 'wareh': 82,\n",
              " 'app': 83,\n",
              " 'katamari': 84,\n",
              " 'well': 85,\n",
              " '…': 86,\n",
              " 'name': 87,\n",
              " 'impli': 88,\n",
              " ':p': 89,\n",
              " 'influenc': 90,\n",
              " 'big': 91,\n",
              " '...': 92,\n",
              " 'juici': 93,\n",
              " 'selfi': 94,\n",
              " 'follow': 95,\n",
              " 'perfect': 96,\n",
              " 'alreadi': 97,\n",
              " 'know': 98,\n",
              " \"what'\": 99,\n",
              " 'great': 100,\n",
              " 'opportun': 101,\n",
              " 'junior': 102,\n",
              " 'triathlet': 103,\n",
              " 'age': 104,\n",
              " '12': 105,\n",
              " '13': 106,\n",
              " 'gatorad': 107,\n",
              " 'seri': 108,\n",
              " 'get': 109,\n",
              " 'entri': 110,\n",
              " 'lay': 111,\n",
              " 'greet': 112,\n",
              " 'card': 113,\n",
              " 'rang': 114,\n",
              " 'print': 115,\n",
              " 'today': 116,\n",
              " 'job': 117,\n",
              " ':-)': 118,\n",
              " \"friend'\": 119,\n",
              " 'lunch': 120,\n",
              " 'yummm': 121,\n",
              " 'nostalgia': 122,\n",
              " 'tb': 123,\n",
              " 'ku': 124,\n",
              " 'id': 125,\n",
              " 'conflict': 126,\n",
              " 'help': 127,\n",
              " \"here'\": 128,\n",
              " 'screenshot': 129,\n",
              " 'work': 130,\n",
              " 'hi': 131,\n",
              " 'liv': 132,\n",
              " 'hello': 133,\n",
              " 'need': 134,\n",
              " 'someth': 135,\n",
              " 'u': 136,\n",
              " 'fm': 137,\n",
              " 'twitter': 138,\n",
              " '—': 139,\n",
              " 'sure': 140,\n",
              " 'thing': 141,\n",
              " 'dm': 142,\n",
              " 'x': 143,\n",
              " \"i'v\": 144,\n",
              " 'heard': 145,\n",
              " 'four': 146,\n",
              " 'season': 147,\n",
              " 'pretti': 148,\n",
              " 'dope': 149,\n",
              " 'penthous': 150,\n",
              " 'obv': 151,\n",
              " 'gobigorgohom': 152,\n",
              " 'fun': 153,\n",
              " \"y'all\": 154,\n",
              " 'yeah': 155,\n",
              " 'suppos': 156,\n",
              " 'lol': 157,\n",
              " 'chat': 158,\n",
              " 'bit': 159,\n",
              " 'youth': 160,\n",
              " '💅🏽': 161,\n",
              " '💋': 162,\n",
              " 'seen': 163,\n",
              " 'year': 164,\n",
              " 'rest': 165,\n",
              " 'goe': 166,\n",
              " 'quickli': 167,\n",
              " 'bed': 168,\n",
              " 'music': 169,\n",
              " 'fix': 170,\n",
              " 'dream': 171,\n",
              " 'spiritu': 172,\n",
              " 'ritual': 173,\n",
              " 'festiv': 174,\n",
              " 'népal': 175,\n",
              " 'begin': 176,\n",
              " 'line-up': 177,\n",
              " 'left': 178,\n",
              " 'see': 179,\n",
              " 'sarah': 180,\n",
              " 'send': 181,\n",
              " 'us': 182,\n",
              " 'email': 183,\n",
              " 'bitsy@bitdefender.com': 184,\n",
              " \"we'll\": 185,\n",
              " 'asap': 186,\n",
              " 'kik': 187,\n",
              " 'hatessuc': 188,\n",
              " '32429': 189,\n",
              " 'kikm': 190,\n",
              " 'lgbt': 191,\n",
              " 'tinder': 192,\n",
              " 'nsfw': 193,\n",
              " 'akua': 194,\n",
              " 'cumshot': 195,\n",
              " 'come': 196,\n",
              " 'hous': 197,\n",
              " 'nsn_supplement': 198,\n",
              " 'effect': 199,\n",
              " 'press': 200,\n",
              " 'releas': 201,\n",
              " 'distribut': 202,\n",
              " 'result': 203,\n",
              " 'link': 204,\n",
              " 'remov': 205,\n",
              " 'pressreleas': 206,\n",
              " 'newsdistribut': 207,\n",
              " 'bam': 208,\n",
              " 'bestfriend': 209,\n",
              " 'lot': 210,\n",
              " 'warsaw': 211,\n",
              " '<3': 212,\n",
              " 'x46': 213,\n",
              " 'everyon': 214,\n",
              " 'watch': 215,\n",
              " 'documentari': 216,\n",
              " 'earthl': 217,\n",
              " 'youtub': 218,\n",
              " 'support': 219,\n",
              " 'buuut': 220,\n",
              " 'oh': 221,\n",
              " 'look': 222,\n",
              " 'forward': 223,\n",
              " 'visit': 224,\n",
              " 'next': 225,\n",
              " 'letsgetmessi': 226,\n",
              " 'jo': 227,\n",
              " 'make': 228,\n",
              " 'feel': 229,\n",
              " 'better': 230,\n",
              " 'never': 231,\n",
              " 'anyon': 232,\n",
              " 'kpop': 233,\n",
              " 'flesh': 234,\n",
              " 'good': 235,\n",
              " 'girl': 236,\n",
              " 'best': 237,\n",
              " 'wish': 238,\n",
              " 'reason': 239,\n",
              " 'epic': 240,\n",
              " 'soundtrack': 241,\n",
              " 'shout': 242,\n",
              " 'ad': 243,\n",
              " 'video': 244,\n",
              " 'playlist': 245,\n",
              " 'would': 246,\n",
              " 'dear': 247,\n",
              " 'jordan': 248,\n",
              " 'okay': 249,\n",
              " 'fake': 250,\n",
              " 'gameplay': 251,\n",
              " ';)': 252,\n",
              " 'haha': 253,\n",
              " 'im': 254,\n",
              " 'kid': 255,\n",
              " 'stuff': 256,\n",
              " 'exactli': 257,\n",
              " 'product': 258,\n",
              " 'line': 259,\n",
              " 'etsi': 260,\n",
              " 'shop': 261,\n",
              " 'check': 262,\n",
              " 'vacat': 263,\n",
              " 'recharg': 264,\n",
              " 'normal': 265,\n",
              " 'charger': 266,\n",
              " 'asleep': 267,\n",
              " 'talk': 268,\n",
              " 'sooo': 269,\n",
              " 'someon': 270,\n",
              " 'text': 271,\n",
              " 'ye': 272,\n",
              " 'bet': 273,\n",
              " \"he'll\": 274,\n",
              " 'fit': 275,\n",
              " 'hear': 276,\n",
              " 'speech': 277,\n",
              " 'piti': 278,\n",
              " 'green': 279,\n",
              " 'garden': 280,\n",
              " 'midnight': 281,\n",
              " 'sun': 282,\n",
              " 'beauti': 283,\n",
              " 'canal': 284,\n",
              " 'dasvidaniya': 285,\n",
              " 'till': 286,\n",
              " 'scout': 287,\n",
              " 'sg': 288,\n",
              " 'futur': 289,\n",
              " 'wlan': 290,\n",
              " 'pro': 291,\n",
              " 'confer': 292,\n",
              " 'asia': 293,\n",
              " 'chang': 294,\n",
              " 'lollipop': 295,\n",
              " '🍭': 296,\n",
              " 'nez': 297,\n",
              " 'agnezmo': 298,\n",
              " 'oley': 299,\n",
              " 'mama': 300,\n",
              " 'stand': 301,\n",
              " 'stronger': 302,\n",
              " 'god': 303,\n",
              " 'misti': 304,\n",
              " 'babi': 305,\n",
              " 'cute': 306,\n",
              " 'woohoo': 307,\n",
              " \"can't\": 308,\n",
              " 'sign': 309,\n",
              " 'yet': 310,\n",
              " 'still': 311,\n",
              " 'think': 312,\n",
              " 'mka': 313,\n",
              " 'liam': 314,\n",
              " 'access': 315,\n",
              " 'welcom': 316,\n",
              " 'stat': 317,\n",
              " 'arriv': 318,\n",
              " '1': 319,\n",
              " 'unfollow': 320,\n",
              " 'via': 321,\n",
              " 'surpris': 322,\n",
              " 'figur': 323,\n",
              " 'happybirthdayemilybett': 324,\n",
              " 'sweet': 325,\n",
              " 'talent': 326,\n",
              " '2': 327,\n",
              " 'plan': 328,\n",
              " 'drain': 329,\n",
              " 'gotta': 330,\n",
              " 'timezon': 331,\n",
              " 'parent': 332,\n",
              " 'proud': 333,\n",
              " 'least': 334,\n",
              " 'mayb': 335,\n",
              " 'sometim': 336,\n",
              " 'grade': 337,\n",
              " 'al': 338,\n",
              " 'grand': 339,\n",
              " 'manila_bro': 340,\n",
              " 'chosen': 341,\n",
              " 'let': 342,\n",
              " 'around': 343,\n",
              " '..': 344,\n",
              " 'side': 345,\n",
              " 'world': 346,\n",
              " 'eh': 347,\n",
              " 'take': 348,\n",
              " 'care': 349,\n",
              " 'final': 350,\n",
              " 'fuck': 351,\n",
              " 'weekend': 352,\n",
              " 'real': 353,\n",
              " 'x45': 354,\n",
              " 'join': 355,\n",
              " 'hushedcallwithfraydo': 356,\n",
              " 'gift': 357,\n",
              " 'yeahhh': 358,\n",
              " 'hushedpinwithsammi': 359,\n",
              " 'event': 360,\n",
              " 'might': 361,\n",
              " 'luv': 362,\n",
              " 'realli': 363,\n",
              " 'appreci': 364,\n",
              " 'share': 365,\n",
              " 'wow': 366,\n",
              " 'tom': 367,\n",
              " 'gym': 368,\n",
              " 'monday': 369,\n",
              " 'invit': 370,\n",
              " 'scope': 371,\n",
              " 'friend': 372,\n",
              " 'nude': 373,\n",
              " 'sleep': 374,\n",
              " 'birthday': 375,\n",
              " 'want': 376,\n",
              " 't-shirt': 377,\n",
              " 'cool': 378,\n",
              " 'haw': 379,\n",
              " 'phela': 380,\n",
              " 'mom': 381,\n",
              " 'obvious': 382,\n",
              " 'princ': 383,\n",
              " 'charm': 384,\n",
              " 'stage': 385,\n",
              " 'luck': 386,\n",
              " 'tyler': 387,\n",
              " 'hipster': 388,\n",
              " 'glass': 389,\n",
              " 'marti': 390,\n",
              " 'glad': 391,\n",
              " 'done': 392,\n",
              " 'afternoon': 393,\n",
              " 'read': 394,\n",
              " 'kahfi': 395,\n",
              " 'finish': 396,\n",
              " 'ohmyg': 397,\n",
              " 'yaya': 398,\n",
              " 'dub': 399,\n",
              " 'stalk': 400,\n",
              " 'ig': 401,\n",
              " 'gondooo': 402,\n",
              " 'moo': 403,\n",
              " 'tologooo': 404,\n",
              " 'becom': 405,\n",
              " 'detail': 406,\n",
              " 'zzz': 407,\n",
              " 'xx': 408,\n",
              " 'physiotherapi': 409,\n",
              " 'hashtag': 410,\n",
              " '💪': 411,\n",
              " 'monica': 412,\n",
              " 'miss': 413,\n",
              " 'sound': 414,\n",
              " 'morn': 415,\n",
              " \"that'\": 416,\n",
              " 'x43': 417,\n",
              " 'definit': 418,\n",
              " 'tri': 419,\n",
              " 'tonight': 420,\n",
              " 'took': 421,\n",
              " 'advic': 422,\n",
              " 'treviso': 423,\n",
              " 'concert': 424,\n",
              " 'citi': 425,\n",
              " 'countri': 426,\n",
              " \"i'll\": 427,\n",
              " 'start': 428,\n",
              " 'fine': 429,\n",
              " 'gorgeou': 430,\n",
              " 'xo': 431,\n",
              " 'oven': 432,\n",
              " 'roast': 433,\n",
              " 'garlic': 434,\n",
              " 'oliv': 435,\n",
              " 'oil': 436,\n",
              " 'dri': 437,\n",
              " 'tomato': 438,\n",
              " 'basil': 439,\n",
              " 'centuri': 440,\n",
              " 'tuna': 441,\n",
              " 'right': 442,\n",
              " 'back': 443,\n",
              " 'atchya': 444,\n",
              " 'even': 445,\n",
              " 'almost': 446,\n",
              " 'chanc': 447,\n",
              " 'cheer': 448,\n",
              " 'po': 449,\n",
              " 'ice': 450,\n",
              " 'cream': 451,\n",
              " 'agre': 452,\n",
              " '100': 453,\n",
              " 'heheheh': 454,\n",
              " 'that': 455,\n",
              " 'point': 456,\n",
              " 'stay': 457,\n",
              " 'home': 458,\n",
              " 'soon': 459,\n",
              " 'promis': 460,\n",
              " 'web': 461,\n",
              " 'whatsapp': 462,\n",
              " 'volta': 463,\n",
              " 'funcionar': 464,\n",
              " 'com': 465,\n",
              " 'iphon': 466,\n",
              " 'jailbroken': 467,\n",
              " 'later': 468,\n",
              " '34': 469,\n",
              " 'min': 470,\n",
              " 'leia': 471,\n",
              " 'appear': 472,\n",
              " 'hologram': 473,\n",
              " 'r2d2': 474,\n",
              " 'w': 475,\n",
              " 'messag': 476,\n",
              " 'obi': 477,\n",
              " 'wan': 478,\n",
              " 'sit': 479,\n",
              " 'luke': 480,\n",
              " 'inter': 481,\n",
              " '3': 482,\n",
              " 'ucl': 483,\n",
              " 'arsen': 484,\n",
              " 'small': 485,\n",
              " 'team': 486,\n",
              " 'pass': 487,\n",
              " '🚂': 488,\n",
              " 'dewsburi': 489,\n",
              " 'railway': 490,\n",
              " 'station': 491,\n",
              " 'dew': 492,\n",
              " 'west': 493,\n",
              " 'yorkshir': 494,\n",
              " '430': 495,\n",
              " 'smh': 496,\n",
              " '9:25': 497,\n",
              " 'live': 498,\n",
              " 'strang': 499,\n",
              " 'imagin': 500,\n",
              " 'megan': 501,\n",
              " 'masaantoday': 502,\n",
              " 'a4': 503,\n",
              " 'shweta': 504,\n",
              " 'tripathi': 505,\n",
              " '5': 506,\n",
              " '20': 507,\n",
              " 'kurta': 508,\n",
              " 'half': 509,\n",
              " 'number': 510,\n",
              " 'wsalelov': 511,\n",
              " 'ah': 512,\n",
              " 'larri': 513,\n",
              " 'anyway': 514,\n",
              " 'kinda': 515,\n",
              " 'goood': 516,\n",
              " 'life': 517,\n",
              " 'enn': 518,\n",
              " 'could': 519,\n",
              " 'warmup': 520,\n",
              " '15th': 521,\n",
              " 'bath': 522,\n",
              " 'dum': 523,\n",
              " 'andar': 524,\n",
              " 'ram': 525,\n",
              " 'sampath': 526,\n",
              " 'sona': 527,\n",
              " 'mohapatra': 528,\n",
              " 'samantha': 529,\n",
              " 'edward': 530,\n",
              " 'mein': 531,\n",
              " 'tulan': 532,\n",
              " 'razi': 533,\n",
              " 'wah': 534,\n",
              " 'josh': 535,\n",
              " 'alway': 536,\n",
              " 'smile': 537,\n",
              " 'pictur': 538,\n",
              " '16.20': 539,\n",
              " 'giveitup': 540,\n",
              " 'given': 541,\n",
              " 'ga': 542,\n",
              " 'subsidi': 543,\n",
              " 'initi': 544,\n",
              " 'propos': 545,\n",
              " 'delight': 546,\n",
              " 'yesterday': 547,\n",
              " 'x42': 548,\n",
              " 'lmaoo': 549,\n",
              " 'song': 550,\n",
              " 'ever': 551,\n",
              " 'shall': 552,\n",
              " 'littl': 553,\n",
              " 'throwback': 554,\n",
              " 'outli': 555,\n",
              " 'island': 556,\n",
              " 'cheung': 557,\n",
              " 'chau': 558,\n",
              " 'mui': 559,\n",
              " 'wo': 560,\n",
              " 'total': 561,\n",
              " 'differ': 562,\n",
              " 'kfckitchentour': 563,\n",
              " 'kitchen': 564,\n",
              " 'clean': 565,\n",
              " \"i'm\": 566,\n",
              " 'cusp': 567,\n",
              " 'test': 568,\n",
              " 'water': 569,\n",
              " 'reward': 570,\n",
              " 'arummzz': 571,\n",
              " \"let'\": 572,\n",
              " 'drive': 573,\n",
              " 'travel': 574,\n",
              " 'yogyakarta': 575,\n",
              " 'jeep': 576,\n",
              " 'indonesia': 577,\n",
              " 'instamood': 578,\n",
              " 'wanna': 579,\n",
              " 'skype': 580,\n",
              " 'may': 581,\n",
              " 'nice': 582,\n",
              " 'friendli': 583,\n",
              " 'pretend': 584,\n",
              " 'film': 585,\n",
              " 'congratul': 586,\n",
              " 'winner': 587,\n",
              " 'cheesydelight': 588,\n",
              " 'contest': 589,\n",
              " 'address': 590,\n",
              " 'guy': 591,\n",
              " 'market': 592,\n",
              " '24/7': 593,\n",
              " '14': 594,\n",
              " 'hour': 595,\n",
              " 'leav': 596,\n",
              " 'without': 597,\n",
              " 'delay': 598,\n",
              " 'actual': 599,\n",
              " 'easi': 600,\n",
              " 'guess': 601,\n",
              " 'train': 602,\n",
              " 'wd': 603,\n",
              " 'shift': 604,\n",
              " 'engin': 605,\n",
              " 'etc': 606,\n",
              " 'sunburn': 607,\n",
              " 'peel': 608,\n",
              " 'blog': 609,\n",
              " 'huge': 610,\n",
              " 'warm': 611,\n",
              " '☆': 612,\n",
              " 'complet': 613,\n",
              " 'triangl': 614,\n",
              " 'northern': 615,\n",
              " 'ireland': 616,\n",
              " 'sight': 617,\n",
              " 'smthng': 618,\n",
              " 'fr': 619,\n",
              " 'hug': 620,\n",
              " 'xoxo': 621,\n",
              " 'uu': 622,\n",
              " 'jaann': 623,\n",
              " 'topnewfollow': 624,\n",
              " 'connect': 625,\n",
              " 'wonder': 626,\n",
              " 'made': 627,\n",
              " 'fluffi': 628,\n",
              " 'insid': 629,\n",
              " 'pirouett': 630,\n",
              " 'moos': 631,\n",
              " 'trip': 632,\n",
              " 'philli': 633,\n",
              " 'decemb': 634,\n",
              " \"i'd\": 635,\n",
              " 'dude': 636,\n",
              " 'x41': 637,\n",
              " 'question': 638,\n",
              " 'flaw': 639,\n",
              " 'pain': 640,\n",
              " 'negat': 641,\n",
              " 'strength': 642,\n",
              " 'went': 643,\n",
              " 'solo': 644,\n",
              " 'move': 645,\n",
              " 'fav': 646,\n",
              " 'nirvana': 647,\n",
              " 'smell': 648,\n",
              " 'teen': 649,\n",
              " 'spirit': 650,\n",
              " 'rip': 651,\n",
              " 'ami': 652,\n",
              " 'winehous': 653,\n",
              " 'coupl': 654,\n",
              " 'tomhiddleston': 655,\n",
              " 'elizabetholsen': 656,\n",
              " 'yaytheylookgreat': 657,\n",
              " 'goodnight': 658,\n",
              " 'vid': 659,\n",
              " 'wake': 660,\n",
              " 'gonna': 661,\n",
              " 'shoot': 662,\n",
              " 'itti': 663,\n",
              " 'bitti': 664,\n",
              " 'teeni': 665,\n",
              " 'bikini': 666,\n",
              " 'much': 667,\n",
              " '4th': 668,\n",
              " 'togeth': 669,\n",
              " 'end': 670,\n",
              " 'xfile': 671,\n",
              " 'content': 672,\n",
              " 'rain': 673,\n",
              " 'fabul': 674,\n",
              " 'fantast': 675,\n",
              " '♡': 676,\n",
              " 'jb': 677,\n",
              " 'forev': 678,\n",
              " 'belieb': 679,\n",
              " 'nighti': 680,\n",
              " 'bug': 681,\n",
              " 'bite': 682,\n",
              " 'bracelet': 683,\n",
              " 'idea': 684,\n",
              " 'foundri': 685,\n",
              " 'game': 686,\n",
              " 'sens': 687,\n",
              " 'pic': 688,\n",
              " 'ef': 689,\n",
              " 'phone': 690,\n",
              " 'woot': 691,\n",
              " 'derek': 692,\n",
              " 'use': 693,\n",
              " 'parkshar': 694,\n",
              " 'gloucestershir': 695,\n",
              " 'aaaahhh': 696,\n",
              " 'man': 697,\n",
              " 'traffic': 698,\n",
              " 'stress': 699,\n",
              " 'reliev': 700,\n",
              " \"how'r\": 701,\n",
              " 'arbeloa': 702,\n",
              " 'turn': 703,\n",
              " '17': 704,\n",
              " 'omg': 705,\n",
              " 'say': 706,\n",
              " 'europ': 707,\n",
              " 'rise': 708,\n",
              " 'find': 709,\n",
              " 'hard': 710,\n",
              " 'believ': 711,\n",
              " 'uncount': 712,\n",
              " 'coz': 713,\n",
              " 'unlimit': 714,\n",
              " 'cours': 715,\n",
              " 'teamposit': 716,\n",
              " 'aldub': 717,\n",
              " '☕': 718,\n",
              " 'rita': 719,\n",
              " 'info': 720,\n",
              " \"we'd\": 721,\n",
              " 'way': 722,\n",
              " 'boy': 723,\n",
              " 'x40': 724,\n",
              " 'true': 725,\n",
              " 'sethi': 726,\n",
              " 'high': 727,\n",
              " 'exe': 728,\n",
              " 'skeem': 729,\n",
              " 'saam': 730,\n",
              " 'peopl': 731,\n",
              " 'polit': 732,\n",
              " 'izzat': 733,\n",
              " 'wese': 734,\n",
              " 'trust': 735,\n",
              " 'khawateen': 736,\n",
              " 'k': 737,\n",
              " 'sath': 738,\n",
              " 'mana': 739,\n",
              " 'kar': 740,\n",
              " 'deya': 741,\n",
              " 'sort': 742,\n",
              " 'smart': 743,\n",
              " 'hair': 744,\n",
              " 'tbh': 745,\n",
              " 'jacob': 746,\n",
              " 'g': 747,\n",
              " 'upgrad': 748,\n",
              " 'tee': 749,\n",
              " 'famili': 750,\n",
              " 'person': 751,\n",
              " 'two': 752,\n",
              " 'convers': 753,\n",
              " 'onlin': 754,\n",
              " 'mclaren': 755,\n",
              " 'fridayfeel': 756,\n",
              " 'tgif': 757,\n",
              " 'squar': 758,\n",
              " 'enix': 759,\n",
              " 'bissmillah': 760,\n",
              " 'ya': 761,\n",
              " 'allah': 762,\n",
              " \"we'r\": 763,\n",
              " 'socent': 764,\n",
              " 'startup': 765,\n",
              " 'drop': 766,\n",
              " 'your': 767,\n",
              " 'arnd': 768,\n",
              " 'town': 769,\n",
              " 'basic': 770,\n",
              " 'piss': 771,\n",
              " 'cup': 772,\n",
              " 'also': 773,\n",
              " 'terribl': 774,\n",
              " 'complic': 775,\n",
              " 'discuss': 776,\n",
              " 'snapchat': 777,\n",
              " 'lynettelow': 778,\n",
              " 'kikmenow': 779,\n",
              " 'snapm': 780,\n",
              " 'hot': 781,\n",
              " 'amazon': 782,\n",
              " 'kikmeguy': 783,\n",
              " 'defin': 784,\n",
              " 'grow': 785,\n",
              " 'sport': 786,\n",
              " 'rt': 787,\n",
              " 'rakyat': 788,\n",
              " 'write': 789,\n",
              " 'sinc': 790,\n",
              " 'mention': 791,\n",
              " 'fli': 792,\n",
              " 'fish': 793,\n",
              " 'promot': 794,\n",
              " 'post': 795,\n",
              " 'cyber': 796,\n",
              " 'ourdaughtersourprid': 797,\n",
              " 'mypapamyprid': 798,\n",
              " 'papa': 799,\n",
              " 'coach': 800,\n",
              " 'posit': 801,\n",
              " 'kha': 802,\n",
              " 'atleast': 803,\n",
              " 'x39': 804,\n",
              " 'mango': 805,\n",
              " \"lassi'\": 806,\n",
              " \"monty'\": 807,\n",
              " 'marvel': 808,\n",
              " 'though': 809,\n",
              " 'suspect': 810,\n",
              " 'meant': 811,\n",
              " '24': 812,\n",
              " 'hr': 813,\n",
              " 'touch': 814,\n",
              " 'kepler': 815,\n",
              " '452b': 816,\n",
              " 'chalna': 817,\n",
              " 'hai': 818,\n",
              " 'thankyou': 819,\n",
              " 'hazel': 820,\n",
              " 'food': 821,\n",
              " 'brooklyn': 822,\n",
              " 'pta': 823,\n",
              " 'awak': 824,\n",
              " 'okayi': 825,\n",
              " 'awww': 826,\n",
              " 'ha': 827,\n",
              " 'doc': 828,\n",
              " 'splendid': 829,\n",
              " 'spam': 830,\n",
              " 'folder': 831,\n",
              " 'amount': 832,\n",
              " 'nigeria': 833,\n",
              " 'claim': 834,\n",
              " 'rted': 835,\n",
              " 'leg': 836,\n",
              " 'hurt': 837,\n",
              " 'bad': 838,\n",
              " 'mine': 839,\n",
              " 'saturday': 840,\n",
              " 'thaaank': 841,\n",
              " 'puhon': 842,\n",
              " 'happinesss': 843,\n",
              " 'tnc': 844,\n",
              " 'prior': 845,\n",
              " 'notif': 846,\n",
              " 'fat': 847,\n",
              " 'co': 848,\n",
              " 'probabl': 849,\n",
              " 'ate': 850,\n",
              " 'yuna': 851,\n",
              " 'tamesid': 852,\n",
              " '´': 853,\n",
              " 'googl': 854,\n",
              " 'account': 855,\n",
              " 'scouser': 856,\n",
              " 'everyth': 857,\n",
              " 'zoe': 858,\n",
              " 'mate': 859,\n",
              " 'liter': 860,\n",
              " \"they'r\": 861,\n",
              " 'samee': 862,\n",
              " 'edgar': 863,\n",
              " 'updat': 864,\n",
              " 'log': 865,\n",
              " 'bring': 866,\n",
              " 'abe': 867,\n",
              " 'meet': 868,\n",
              " 'x38': 869,\n",
              " 'sigh': 870,\n",
              " 'dreamili': 871,\n",
              " 'pout': 872,\n",
              " 'eye': 873,\n",
              " 'quacketyquack': 874,\n",
              " 'funni': 875,\n",
              " 'happen': 876,\n",
              " 'phil': 877,\n",
              " 'em': 878,\n",
              " 'del': 879,\n",
              " 'rodder': 880,\n",
              " 'els': 881,\n",
              " 'play': 882,\n",
              " 'newest': 883,\n",
              " 'gamejam': 884,\n",
              " 'irish': 885,\n",
              " 'literatur': 886,\n",
              " 'inaccess': 887,\n",
              " \"kareena'\": 888,\n",
              " 'fan': 889,\n",
              " 'brain': 890,\n",
              " 'dot': 891,\n",
              " 'braindot': 892,\n",
              " 'fair': 893,\n",
              " 'rush': 894,\n",
              " 'either': 895,\n",
              " 'brandi': 896,\n",
              " '18': 897,\n",
              " 'carniv': 898,\n",
              " 'men': 899,\n",
              " 'put': 900,\n",
              " 'mask': 901,\n",
              " 'xavier': 902,\n",
              " 'forneret': 903,\n",
              " 'jennif': 904,\n",
              " 'site': 905,\n",
              " 'free': 906,\n",
              " '50.000': 907,\n",
              " '8': 908,\n",
              " 'ball': 909,\n",
              " 'pool': 910,\n",
              " 'coin': 911,\n",
              " 'edit': 912,\n",
              " 'trish': 913,\n",
              " '♥': 914,\n",
              " 'grate': 915,\n",
              " 'three': 916,\n",
              " 'comment': 917,\n",
              " 'wakeup': 918,\n",
              " 'besid': 919,\n",
              " 'dirti': 920,\n",
              " 'sex': 921,\n",
              " 'lmaooo': 922,\n",
              " '😤': 923,\n",
              " 'loui': 924,\n",
              " \"he'\": 925,\n",
              " 'throw': 926,\n",
              " 'caus': 927,\n",
              " 'inspir': 928,\n",
              " 'ff': 929,\n",
              " 'twoof': 930,\n",
              " 'gr8': 931,\n",
              " 'wkend': 932,\n",
              " 'kind': 933,\n",
              " 'exhaust': 934,\n",
              " 'word': 935,\n",
              " 'cheltenham': 936,\n",
              " 'area': 937,\n",
              " 'kale': 938,\n",
              " 'crisp': 939,\n",
              " 'ruin': 940,\n",
              " 'x37': 941,\n",
              " 'open': 942,\n",
              " 'worldwid': 943,\n",
              " 'outta': 944,\n",
              " 'sfvbeta': 945,\n",
              " 'vantast': 946,\n",
              " 'xcylin': 947,\n",
              " 'bundl': 948,\n",
              " 'show': 949,\n",
              " 'internet': 950,\n",
              " 'price': 951,\n",
              " 'realisticli': 952,\n",
              " 'pay': 953,\n",
              " 'net': 954,\n",
              " 'educ': 955,\n",
              " 'power': 956,\n",
              " 'weapon': 957,\n",
              " 'nelson': 958,\n",
              " 'mandela': 959,\n",
              " 'recent': 960,\n",
              " 'j': 961,\n",
              " 'chenab': 962,\n",
              " 'flow': 963,\n",
              " 'pakistan': 964,\n",
              " 'incredibleindia': 965,\n",
              " 'teenchoic': 966,\n",
              " 'choiceinternationalartist': 967,\n",
              " 'superjunior': 968,\n",
              " 'caught': 969,\n",
              " 'first': 970,\n",
              " 'salmon': 971,\n",
              " 'super-blend': 972,\n",
              " 'project': 973,\n",
              " 'youth@bipolaruk.org.uk': 974,\n",
              " 'awesom': 975,\n",
              " 'stream': 976,\n",
              " 'alma': 977,\n",
              " 'mater': 978,\n",
              " 'highschoolday': 979,\n",
              " 'clientvisit': 980,\n",
              " 'faith': 981,\n",
              " 'christian': 982,\n",
              " 'school': 983,\n",
              " 'lizaminnelli': 984,\n",
              " 'upcom': 985,\n",
              " 'uk': 986,\n",
              " '😄': 987,\n",
              " 'singl': 988,\n",
              " 'hill': 989,\n",
              " 'everi': 990,\n",
              " 'beat': 991,\n",
              " 'wrong': 992,\n",
              " 'readi': 993,\n",
              " 'natur': 994,\n",
              " 'pefumeri': 995,\n",
              " 'workshop': 996,\n",
              " 'neal': 997,\n",
              " 'yard': 998,\n",
              " 'covent': 999,\n",
              " ...}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet - A string containing a tweet\n",
        "        vocab_dict - The words dictionary\n",
        "        unk_token - The special string for unknown tokens\n",
        "        verbose - Print info durign runtime\n",
        "    Output:\n",
        "        tensor_l - A python list with\n",
        "        \n",
        "    '''     \n",
        "    # Process the tweet into a list of words\n",
        "    # where only important words are kept (stop words removed)\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"List of words from the processed tweet:\")\n",
        "        print(word_l)\n",
        "        \n",
        "    # Initialize the list that will contain the unique integer IDs of each word\n",
        "    tensor_l = [] \n",
        "    \n",
        "    # Get the unique integer ID of the __UNK__ token\n",
        "    unk_ID = vocab_dict[unk_token]\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
        "        \n",
        "    # for each word in the list:\n",
        "    for word in word_l:\n",
        "        \n",
        "        # Get the unique integer ID.\n",
        "        # If the word doesn't exist in the vocab dictionary,\n",
        "        # use the unique ID for __UNK__ instead.        \n",
        "        word_ID = vocab_dict[word] if word in vocab_dict else unk_ID\n",
        "            \n",
        "        # Append the unique integer ID to the tensor list.\n",
        "        tensor_l.append(word_ID)\n",
        "    \n",
        "    return tensor_l"
      ],
      "metadata": {
        "id": "PMBnK-28ucKw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Actual tweet is\\n\", val_pos[0])\n",
        "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIoR-Z2zulNI",
        "outputId": "8931abd2-d45b-4b2a-f4d8-577e3e0101f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual tweet is\n",
            " Bro:U wan cut hair anot,ur hair long Liao bo\n",
            "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
            "Bro:LOL Sibei xialan\n",
            "\n",
            "Tensor of tweet:\n",
            " [1064, 136, 478, 2351, 744, 8149, 1122, 744, 53, 2, 2671, 790, 2, 2, 348, 600, 2, 3488, 1016, 596, 4558, 9, 1064, 157, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
        "    '''\n",
        "    Input: \n",
        "        data_pos - Set of positive examples\n",
        "        data_neg - Set of negative examples\n",
        "        batch_size - number of samples per batch. Must be even\n",
        "        loop - True or False\n",
        "        vocab_dict - The words dictionary\n",
        "        shuffle - Shuffle the data order\n",
        "    Yield:\n",
        "        inputs - Subset of positive and negative examples\n",
        "        targets - The corresponding labels for the subset\n",
        "        example_weights - An array specifying the importance of each example\n",
        "        \n",
        "    '''     \n",
        "\n",
        "    # make sure the batch size is an even number\n",
        "    # to allow an equal number of positive and negative samples    \n",
        "    assert batch_size % 2 == 0\n",
        "    \n",
        "    # Number of positive examples in each batch is half of the batch size\n",
        "    # same with number of negative examples in each batch\n",
        "    n_to_take = batch_size // 2\n",
        "    \n",
        "    # Use pos_index to walk through the data_pos array\n",
        "    # same with neg_index and data_neg\n",
        "    pos_index = 0\n",
        "    neg_index = 0\n",
        "    \n",
        "    len_data_pos = len(data_pos)\n",
        "    len_data_neg = len(data_neg)\n",
        "    \n",
        "    # Get and array with the data indexes\n",
        "    pos_index_lines = list(range(len_data_pos))\n",
        "    neg_index_lines = list(range(len_data_neg))\n",
        "    \n",
        "    # shuffle lines if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(pos_index_lines)\n",
        "        rnd.shuffle(neg_index_lines)\n",
        "        \n",
        "    stop = False\n",
        "    \n",
        "    # Loop indefinitely\n",
        "    while not stop:  \n",
        "        \n",
        "        # create a batch with positive and negative examples\n",
        "        batch = []\n",
        "        \n",
        "        # First part: Pack n_to_take positive examples\n",
        "        \n",
        "        # Start from 0 and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "                    \n",
        "            # If the positive index goes past the positive dataset,\n",
        "            if pos_index >= len_data_pos: \n",
        "                \n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True;\n",
        "                    break;\n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                pos_index = 0\n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the positive sample\n",
        "                    rnd.shuffle(pos_index_lines)\n",
        "                    \n",
        "            # get the tweet as pos_index\n",
        "            tweet = data_pos[pos_index_lines[pos_index]]\n",
        "            \n",
        "            # convert the tweet into tensors of integers representing the processed words\n",
        "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "            \n",
        "            # append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "            \n",
        "            # Increment pos_index by one\n",
        "            pos_index = pos_index + 1\n",
        "\n",
        "\n",
        "            \n",
        "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "\n",
        "        # Second part: Pack n_to_take negative examples\n",
        "\n",
        "        # Using the same batch list, start from 0 and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "            \n",
        "            # If the negative index goes past the negative dataset,\n",
        "            if pos_index>=len_data_pos:\n",
        "                \n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True \n",
        "                    break \n",
        "                    \n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                neg_index = 0\n",
        "                \n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the negative sample\n",
        "                    rnd.shuffle(neg_index_lines)\n",
        "                    \n",
        "            # get the tweet as neg_index\n",
        "            tweet = data_neg[neg_index_lines[neg_index]]\n",
        "            \n",
        "            # convert the tweet into tensors of integers representing the processed words\n",
        "            tensor = tweet_to_tensor(tweet,vocab_dict)\n",
        "            \n",
        "            # append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "            \n",
        "            # Increment neg_index by one\n",
        "            neg_index = neg_index +1\n",
        "\n",
        "        ### END CODE HERE ###        \n",
        "\n",
        "        if stop:\n",
        "            break;\n",
        "\n",
        "        # Get the max tweet length (the length of the longest tweet) \n",
        "        # (you will pad all shorter tweets to have this length)\n",
        "        max_len = max([len(t) for t in batch]) \n",
        "        \n",
        "        \n",
        "        # Initialize the input_l, which will \n",
        "        # store the padded versions of the tensors\n",
        "        tensor_pad_l = []\n",
        "        # Pad shorter tweets with zeros\n",
        "        for tensor in batch:\n",
        "\n",
        "\n",
        "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
        "            n_pad = max_len-len(tensor)\n",
        "            \n",
        "            # Generate a list of zeros, with length n_pad\n",
        "            pad_l = [0]*n_pad\n",
        "            \n",
        "            # concatenate the tensor and the list of padded zeros\n",
        "            tensor_pad = tensor+pad_l\n",
        "            \n",
        "            # append the padded tensor to the list of padded tensors\n",
        "            tensor_pad_l.append(tensor_pad)\n",
        "\n",
        "        # convert the list of padded tensors to a numpy array\n",
        "        # and store this as the model inputs\n",
        "        inputs = np.array(tensor_pad_l)\n",
        "  \n",
        "        # Generate the list of targets for the positive examples (a list of ones)\n",
        "        # The length is the number of positive examples in the batch\n",
        "        target_pos = n_to_take*[1]\n",
        "        \n",
        "        # Generate the list of targets for the negative examples (a list of zeros)\n",
        "        # The length is the number of negative examples in the batch\n",
        "        target_neg = n_to_take*[0]\n",
        "        \n",
        "        # Concatenate the positve and negative targets\n",
        "        target_l = target_pos+target_neg\n",
        "        \n",
        "        # Convert the target list into a numpy array\n",
        "        targets = np.array(target_l)\n",
        "\n",
        "        # Example weights: Treat all examples equally importantly.\n",
        "        example_weights = np.ones_like(targets)\n",
        "        \n",
        "\n",
        "\n",
        "        # note we use yield and not return\n",
        "        yield inputs, targets, example_weights"
      ],
      "metadata": {
        "id": "Ln7lCUuxumzD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random number generator for the shuffle procedure\n",
        "rnd.seed(30) \n",
        "\n",
        "# Create the training data generator\n",
        "\n",
        "def train_generator(batch_size, train_pos\n",
        "                    , train_neg, vocab_dict, loop=True\n",
        "                    , shuffle = False):\n",
        "    return data_generator(train_pos, train_neg, batch_size, loop, vocab_dict, shuffle)\n",
        "\n",
        "# Create the validation data generator\n",
        "def val_generator(batch_size, val_pos\n",
        "                    , val_neg, vocab_dict, loop=True\n",
        "                    , shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, loop, vocab_dict, shuffle)\n",
        "\n",
        "# Create the validation data generator\n",
        "def test_generator(batch_size, val_pos\n",
        "                    , val_neg, vocab_dict, loop=False\n",
        "                    , shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, loop, vocab_dict, shuffle)\n",
        "\n",
        "# Get a batch from the train_generator and inspect.\n",
        "inputs, targets, example_weights = next(train_generator(4, train_pos, train_neg, Vocab, shuffle=True))\n",
        "\n",
        "# this will print a list of 4 tensors padded with zeros\n",
        "print(f'Inputs: {inputs}')\n",
        "print(f'Targets: {targets}')\n",
        "print(f'Example Weights: {example_weights}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVBZt4MWuuB6",
        "outputId": "977537af-5f58-4b26-a80b-eef9450b13da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: [[2005 4450 3200    9    0    0    0    0    0    0    0]\n",
            " [4953  566 2000 1453 5173 3498  141 3498  130  458    9]\n",
            " [3760  109  136  582 2929 3968    0    0    0    0    0]\n",
            " [ 249 3760    0    0    0    0    0    0    0    0    0]]\n",
            "Targets: [1 1 0 0]\n",
            "Example Weights: [1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the train_generator\n",
        "\n",
        "# Create a data generator for training data,\n",
        "# which produces batches of size 4 (for tensors and their respective targets)\n",
        "tmp_data_gen = train_generator(batch_size = 4, train_pos=train_pos, train_neg=train_neg, vocab_dict=Vocab)\n",
        "\n",
        "# Call the data generator to get one batch and its targets\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
        "\n",
        "print(f\"The inputs shape is {tmp_inputs.shape}\")\n",
        "for i,t in enumerate(tmp_inputs):\n",
        "    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jJgNGamuxHW",
        "outputId": "f58732ed-988a-4588-f591-45de04955a1c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The inputs shape is (4, 14)\n",
            "input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1\n",
            "input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1\n",
            "input tensor: [5737 2900 3760    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n",
            "input tensor: [ 857  255 3651 5738  306 4457  566 1229 2766  327 1201 3760    0    0]; target 0; example weights 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining classes"
      ],
      "metadata": {
        "id": "Ffi8Yqumu4Eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  ReLU class"
      ],
      "metadata": {
        "id": "z28WiXwNvjJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu(Layer):\n",
        "    \"\"\"Relu activation function implementation\"\"\"\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Input: \n",
        "            - x (a numpy array): the input\n",
        "        Output:\n",
        "            - activation (numpy array): all positive or 0 version of x\n",
        "        '''\n",
        "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "        \n",
        "        activation = np.maximum(0,x)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        return activation"
      ],
      "metadata": {
        "id": "YVMkN0houzFx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dense Class"
      ],
      "metadata": {
        "id": "5fQlhgWLvzoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense(Layer):\n",
        "    \"\"\"\n",
        "    A dense (fully-connected) layer.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, n_units, init_stdev=0.1):\n",
        "        \n",
        "        # Set the number of units in this layer\n",
        "        self._n_units = n_units\n",
        "        self._init_stdev = init_stdev\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        \n",
        "\n",
        "        # Matrix multiply x and the weight matrix\n",
        "        dense = np.dot(x,self.weights)\n",
        "        \n",
        "        \n",
        "        return dense\n",
        "\n",
        "    # init_weights\n",
        "    def init_weights_and_state(self, input_signature, random_key):\n",
        "        \n",
        "        \n",
        "        # The input_signature has a .shape attribute that gives the shape as a tuple\n",
        "        input_shape = input_signature.shape\n",
        "\n",
        "        # Generate the weight matrix from a normal distribution, \n",
        "        # and standard deviation of 'stdev'        \n",
        "        w = self._init_stdev * trax.fastmath.random.normal(key = random_key, shape = (input_shape[-1], self._n_units))\n",
        "                    \n",
        "        self.weights = w\n",
        "        return self.weights"
      ],
      "metadata": {
        "id": "KsfRUEU0vk4L"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Dense layer \n",
        "dense_layer = Dense(n_units=10)  #sets  number of units in dense layer\n",
        "random_key = trax.fastmath.random.get_prng(seed=0)  # sets random seed\n",
        "z = np.array([[2.0, 7.0, 25.0]]) # input array \n",
        "\n",
        "dense_layer.init(z, random_key)\n",
        "print(\"Weights are\\n \",dense_layer.weights) #Returns randomly generated weights\n",
        "print(\"Foward function output is \", dense_layer(z)) # Returns multiplied values of units and weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRRZM3-Jv48j",
        "outputId": "2d963c59-1d16-48ed-dcd6-94d6549c3bd1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights are\n",
            "  [[-0.02837107  0.09368163 -0.10050073  0.14165013  0.10543301  0.09108127\n",
            "  -0.04265671  0.0986188  -0.05575324  0.0015325 ]\n",
            " [-0.2078568   0.05548371  0.09142365  0.05744596  0.07227863  0.01210618\n",
            "  -0.03237354  0.16234998  0.02450039 -0.13809781]\n",
            " [-0.06111237  0.01403725  0.08410043 -0.10943579 -0.1077502  -0.11396457\n",
            "  -0.0593338  -0.01557651 -0.03832145 -0.11144515]]\n",
            "Foward function output is  [[-3.0395489   0.92668045  2.5414748  -2.0504727  -1.9769385  -2.5822086\n",
            "  -1.7952732   0.94427466 -0.89803994 -3.7497485 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement the classifier function"
      ],
      "metadata": {
        "id": "CorO3_7DwTTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier(vocab_size=9088, embedding_dim=256, output_dim=2, mode='train'):\n",
        "    \n",
        "        \n",
        "    # create embedding layer\n",
        "    embed_layer = tl.Embedding( \n",
        "        vocab_size=vocab_size, # Size of the vocabulary\n",
        "        d_feature=embedding_dim # Embedding dimension\n",
        "    ) \n",
        "    \n",
        "    # Create a mean layer, to create an \"average\" word embedding\n",
        "    mean_layer = tl.Mean(axis=1)\n",
        "    \n",
        "    # Create a dense layer, one unit for each output\n",
        "    dense_output_layer = tl.Dense(n_units = output_dim)\n",
        "    \n",
        "    # Create the log softmax layer (no parameters needed)\n",
        "    log_softmax_layer = tl.LogSoftmax()\n",
        "    \n",
        "    # Use tl.Serial to combine all layers\n",
        "    # and create the classifier\n",
        "    # of type trax.layers.combinators.Serial\n",
        "    model = tl.Serial( \n",
        "      embed_layer, # embedding layer\n",
        "      mean_layer, # mean layer\n",
        "      dense_output_layer, # dense output layer\n",
        "      log_softmax_layer # log softmax layer\n",
        "    ) \n",
        "    \n",
        "    \n",
        "    # return the model of type\n",
        "    return model"
      ],
      "metadata": {
        "id": "wynf706fwBW2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_model = classifier(vocab_size=len(Vocab))"
      ],
      "metadata": {
        "id": "LnQfjMedwcZ3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tmp_model))\n",
        "display(tmp_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "Z6xFAt5ewd5W",
        "outputId": "f34bb584-f213-4b5d-db54-9f9dfb256ff2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'trax.layers.combinators.Serial'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Serial[\n",
              "  Embedding_9089_256\n",
              "  Mean\n",
              "  Dense_2\n",
              "  LogSoftmax\n",
              "]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "evOiEuqSwhaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1  Training the model"
      ],
      "metadata": {
        "id": "OaO9xVpGwmE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "def get_train_eval_tasks(train_pos, train_neg, val_pos, val_neg, vocab_dict, loop, batch_size = 16):\n",
        "    \n",
        "    rnd.seed(271)\n",
        "\n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=train_generator(batch_size, train_pos\n",
        "                    , train_neg, vocab_dict, loop\n",
        "                    , shuffle = True),\n",
        "        loss_layer=tl.WeightedCategoryCrossEntropy(),\n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "        n_steps_per_checkpoint=10,\n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data=val_generator(batch_size, val_pos\n",
        "                    , val_neg, vocab_dict, loop\n",
        "                    , shuffle = True),        \n",
        "        metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()],\n",
        "    )\n",
        "    \n",
        "    return train_task, eval_task\n",
        "    \n",
        "\n",
        "train_task, eval_task = get_train_eval_tasks(train_pos, train_neg, val_pos, val_neg, Vocab, True, batch_size = 16)\n",
        "model = classifier()"
      ],
      "metadata": {
        "id": "4C8GbnwNwfHb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTGpdkCzwqOP",
        "outputId": "ed9edfe8-ab0b-4962-afde-fb6bc11f0675"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Serial[\n",
              "  Embedding_9088_256\n",
              "  Mean\n",
              "  Dense_2\n",
              "  LogSoftmax\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = './model/'\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(dir_path)\n",
        "except OSError as e:\n",
        "    pass\n",
        "\n",
        "\n",
        "output_dir = './model/'\n",
        "output_dir_expand = os.path.expanduser(output_dir)\n",
        "print(output_dir_expand)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_YbBCfNwreu",
        "outputId": "16b4d55d-7bc1-4b95-e40a-b344c94e6b8b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./model/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
        "    '''\n",
        "    Input: \n",
        "        classifier - the model you are building\n",
        "        train_task - Training task\n",
        "        eval_task - Evaluation task. Received as a list.\n",
        "        n_steps - the evaluation steps\n",
        "        output_dir - folder to save your files\n",
        "    Output:\n",
        "        trainer -  trax trainer\n",
        "    '''\n",
        "    rnd.seed(31)\n",
        "    \n",
        "        \n",
        "    training_loop = training.Loop( \n",
        "                                classifier, # The learning model\n",
        "                                train_task, # The training task\n",
        "                                eval_tasks=eval_task, # The evaluation task\n",
        "                                output_dir=output_dir, # The output directory\n",
        "                                random_seed=31 # Do not modify this random seed in order to ensure reproducibility and for grading purposes.\n",
        "    ) \n",
        "\n",
        "    training_loop.run(n_steps = n_steps)\n",
        "  \n",
        "    \n",
        "    # Return the training_loop, since it has the model.\n",
        "    return training_loop"
      ],
      "metadata": {
        "id": "AkyZ-440wtYw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop = train_model(model, train_task, [eval_task], 100, output_dir_expand)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tue80WMyJYR",
        "outputId": "175f9efd-6843-45e9-d94f-772a223ba1dd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:554: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 2327042\n",
            "Step      1: Ran 1 train steps in 1.03 secs\n",
            "Step      1: train WeightedCategoryCrossEntropy |  0.68989831\n",
            "Step      1: eval  WeightedCategoryCrossEntropy |  0.69806957\n",
            "Step      1: eval      WeightedCategoryAccuracy |  0.43750000\n",
            "\n",
            "Step     10: Ran 9 train steps in 4.00 secs\n",
            "Step     10: train WeightedCategoryCrossEntropy |  0.64247584\n",
            "Step     10: eval  WeightedCategoryCrossEntropy |  0.53418100\n",
            "Step     10: eval      WeightedCategoryAccuracy |  0.93750000\n",
            "\n",
            "Step     20: Ran 10 train steps in 1.55 secs\n",
            "Step     20: train WeightedCategoryCrossEntropy |  0.45600957\n",
            "Step     20: eval  WeightedCategoryCrossEntropy |  0.33223987\n",
            "Step     20: eval      WeightedCategoryAccuracy |  1.00000000\n",
            "\n",
            "Step     30: Ran 10 train steps in 1.19 secs\n",
            "Step     30: train WeightedCategoryCrossEntropy |  0.24014239\n",
            "Step     30: eval  WeightedCategoryCrossEntropy |  0.15884452\n",
            "Step     30: eval      WeightedCategoryAccuracy |  1.00000000\n",
            "\n",
            "Step     40: Ran 10 train steps in 0.83 secs\n",
            "Step     40: train WeightedCategoryCrossEntropy |  0.13276729\n",
            "Step     40: eval  WeightedCategoryCrossEntropy |  0.06164267\n",
            "Step     40: eval      WeightedCategoryAccuracy |  1.00000000\n",
            "\n",
            "Step     50: Ran 10 train steps in 1.21 secs\n",
            "Step     50: train WeightedCategoryCrossEntropy |  0.08444290\n",
            "Step     50: eval  WeightedCategoryCrossEntropy |  0.06003656\n",
            "Step     50: eval      WeightedCategoryAccuracy |  1.00000000\n",
            "\n",
            "Step     60: Ran 10 train steps in 0.84 secs\n",
            "Step     60: train WeightedCategoryCrossEntropy |  0.04531727\n",
            "Step     60: eval  WeightedCategoryCrossEntropy |  0.02509753\n",
            "Step     60: eval      WeightedCategoryAccuracy |  1.00000000\n",
            "\n",
            "Step     70: Ran 10 train steps in 0.84 secs\n",
            "Step     70: train WeightedCategoryCrossEntropy |  0.03989115\n",
            "Step     70: eval  WeightedCategoryCrossEntropy |  0.00249659\n",
            "Step     70: eval      WeightedCategoryAccuracy |  1.00000000\n",
            "\n",
            "Step     80: Ran 10 train steps in 0.87 secs\n",
            "Step     80: train WeightedCategoryCrossEntropy |  0.01885000\n",
            "Step     80: eval  WeightedCategoryCrossEntropy |  0.00504306\n",
            "Step     80: eval      WeightedCategoryAccuracy |  1.00000000\n",
            "\n",
            "Step     90: Ran 10 train steps in 0.87 secs\n",
            "Step     90: train WeightedCategoryCrossEntropy |  0.04065781\n",
            "Step     90: eval  WeightedCategoryCrossEntropy |  0.00822989\n",
            "Step     90: eval      WeightedCategoryAccuracy |  1.00000000\n",
            "\n",
            "Step    100: Ran 10 train steps in 1.31 secs\n",
            "Step    100: train WeightedCategoryCrossEntropy |  0.01506269\n",
            "Step    100: eval  WeightedCategoryCrossEntropy |  0.09649467\n",
            "Step    100: eval      WeightedCategoryAccuracy |  0.93750000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a generator object\n",
        "tmp_train_generator = train_generator(16, train_pos\n",
        "                    , train_neg, Vocab, loop=True\n",
        "                    , shuffle = False)\n",
        "\n",
        "\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_train_generator)\n",
        "\n",
        "# Position 0 has the model inputs (tweets as tensors)\n",
        "# position 1 has the targets (the actual labels)\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
        "\n",
        "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
        "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
        "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
        "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekaBVZIWyPAV",
        "outputId": "7f6cab36-d045-456c-e709-e68875cb1ffb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.\n",
            "The shape of the tweet tensors is (16, 15) (num of examples, length of tweet tensors)\n",
            "The shape of the labels is (16,), which is the batch size.\n",
            "The shape of the example_weights is (16,), which is the same as inputs/targets size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# feed the tweet tensors into the model to get a prediction\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
        "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
        "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
        "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
        "print()\n",
        "print(\"View the prediction array\")\n",
        "tmp_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsRoEl7WyZbU",
        "outputId": "4d350693-01a7-4664-d2fe-0d829cc9f87c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
            "Column 0 is the probability of a negative sentiment (class 0)\n",
            "Column 1 is the probability of a positive sentiment (class 1)\n",
            "\n",
            "View the prediction array\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-9.5181189e+00, -7.3432922e-05],\n",
              "             [-7.9278040e+00, -3.6072731e-04],\n",
              "             [-1.0872702e+01, -1.9073486e-05],\n",
              "             [-7.3449020e+00, -6.4611435e-04],\n",
              "             [-5.5219040e+00, -4.0061474e-03],\n",
              "             [-8.2074986e+00, -2.7275085e-04],\n",
              "             [-9.1085091e+00, -1.1062622e-04],\n",
              "             [-7.3503170e+00, -6.4253807e-04],\n",
              "             [-2.3729801e-03, -6.0448217e+00],\n",
              "             [-3.0136108e-04, -8.1069393e+00],\n",
              "             [-1.1029243e-03, -6.8103848e+00],\n",
              "             [-1.9073486e-06, -1.3039823e+01],\n",
              "             [-2.3144841e-02, -3.7775359e+00],\n",
              "             [-5.7449341e-03, -5.1623106e+00],\n",
              "             [-2.2547245e-03, -6.0958138e+00],\n",
              "             [-1.5783310e-04, -8.7545242e+00]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turn probabilites into category predictions\n",
        "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
        "for i, p in enumerate(tmp_is_positive):\n",
        "    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eYRRu86ya-o",
        "outputId": "86b9f365-df70-421f-d3a7-c27865052edc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neg log prob -9.5181\tPos log prob -0.0001\t is positive? True\t actual 1\n",
            "Neg log prob -7.9278\tPos log prob -0.0004\t is positive? True\t actual 1\n",
            "Neg log prob -10.8727\tPos log prob -0.0000\t is positive? True\t actual 1\n",
            "Neg log prob -7.3449\tPos log prob -0.0006\t is positive? True\t actual 1\n",
            "Neg log prob -5.5219\tPos log prob -0.0040\t is positive? True\t actual 1\n",
            "Neg log prob -8.2075\tPos log prob -0.0003\t is positive? True\t actual 1\n",
            "Neg log prob -9.1085\tPos log prob -0.0001\t is positive? True\t actual 1\n",
            "Neg log prob -7.3503\tPos log prob -0.0006\t is positive? True\t actual 1\n",
            "Neg log prob -0.0024\tPos log prob -6.0448\t is positive? False\t actual 0\n",
            "Neg log prob -0.0003\tPos log prob -8.1069\t is positive? False\t actual 0\n",
            "Neg log prob -0.0011\tPos log prob -6.8104\t is positive? False\t actual 0\n",
            "Neg log prob -0.0000\tPos log prob -13.0398\t is positive? False\t actual 0\n",
            "Neg log prob -0.0231\tPos log prob -3.7775\t is positive? False\t actual 0\n",
            "Neg log prob -0.0057\tPos log prob -5.1623\t is positive? False\t actual 0\n",
            "Neg log prob -0.0023\tPos log prob -6.0958\t is positive? False\t actual 0\n",
            "Neg log prob -0.0002\tPos log prob -8.7545\t is positive? False\t actual 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View the array of booleans\n",
        "print(\"Array of booleans\")\n",
        "display(tmp_is_positive)\n",
        "\n",
        "# convert boolean to type int32\n",
        "# True is converted to 1\n",
        "# False is converted to 0\n",
        "tmp_is_positive_int = tmp_is_positive.astype(np.int32)\n",
        "\n",
        "\n",
        "# View the array of integers\n",
        "print(\"Array of integers\")\n",
        "display(tmp_is_positive_int)\n",
        "\n",
        "# convert boolean to type float32\n",
        "tmp_is_positive_float = tmp_is_positive.astype(np.float32)\n",
        "\n",
        "# View the array of floats\n",
        "print(\"Array of floats\")\n",
        "display(tmp_is_positive_float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "XelQdT-PydEw",
        "outputId": "b9733eaa-6340-4c88-fc79-787ca8273fb9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array of booleans\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "             False, False, False, False, False, False, False, False],            dtype=bool)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array of integers\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DeviceArray([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array of floats\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DeviceArray([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "             0.], dtype=float32)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"True == 1: {True == 1}\")\n",
        "print(f\"True == 2: {True == 2}\")\n",
        "print(f\"False == 0: {False == 0}\")\n",
        "print(f\"False == 2: {False == 2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GdAhfD7ykCo",
        "outputId": "e41c7f5f-c7b5-4dfe-d1b2-f25d0bd975ab"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True == 1: True\n",
            "True == 2: False\n",
            "False == 0: True\n",
            "False == 2: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "cwJZ36Oqyp6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(preds, y, y_weights):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        preds: a tensor of shape (dim_batch, output_dim) \n",
        "        y: a tensor of shape (dim_batch, output_dim) with the true labels\n",
        "        y_weights: a n.ndarray with the a weight for each example\n",
        "    Output: \n",
        "        accuracy: a float between 0-1 \n",
        "        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n",
        "        sum_weights (np.float32): Sum of the weights\n",
        "    \"\"\"\n",
        "    # Create an array of booleans, \n",
        "    # True if the probability of positive sentiment is greater than\n",
        "    # the probability of negative sentiment\n",
        "    # else False\n",
        "    is_pos =  preds[:,1] > preds[:,0]\n",
        "\n",
        "    # convert the array of booleans into an array of np.int32\n",
        "    is_pos_int = is_pos.astype(np.int32)\n",
        "    \n",
        "    # compare the array of predictions (as int32) with the target (labels) of type int32\n",
        "    correct = is_pos_int == y\n",
        "\n",
        "    # Count the sum of the weights.\n",
        "    sum_weights = np.sum(y_weights)\n",
        "    \n",
        "    # convert the array of correct predictions (boolean) into an arrayof np.float32\n",
        "    correct_float = correct.astype(np.float32)\n",
        "    \n",
        "    # Multiply each prediction with its corresponding weight.\n",
        "    weighted_correct_float = correct_float * y_weights\n",
        "\n",
        "    # Sum up the weighted correct predictions (of type np.float32), to go in the\n",
        "    # denominator.\n",
        "    weighted_num_correct = np.sum(weighted_correct_float)\n",
        " \n",
        "    # Divide the number of weighted correct predictions by the sum of the\n",
        "    # weights.\n",
        "    accuracy = weighted_num_correct / sum_weights\n",
        "\n",
        "    return accuracy, weighted_num_correct, sum_weights"
      ],
      "metadata": {
        "id": "bvncahY0ymov"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test function\n",
        "tmp_val_generator = val_generator(64, val_pos\n",
        "                    , val_neg, Vocab, loop=True\n",
        "                    , shuffle = False)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_val_generator)\n",
        "\n",
        "# Position 0 has the model inputs (tweets as tensors)\n",
        "# position 1 has the targets (the actual labels)\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
        "\n",
        "# feed the tweet tensors into the model to get a prediction\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
        "tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
        "\n",
        "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
        "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWZQMH5fywS1",
        "outputId": "59d15c08-e2f9-468b-e747-f2d5bcf3f718"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's prediction accuracy on a single training batch is: 100.0%\n",
            "Weighted number of correct predictions 64.0; weighted number of total observations predicted 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(generator, model, compute_accuracy=compute_accuracy):\n",
        "    '''\n",
        "    Input: \n",
        "        generator: an iterator instance that provides batches of inputs and targets\n",
        "        model: a model instance \n",
        "    Output: \n",
        "        accuracy: float corresponding to the accuracy\n",
        "    '''\n",
        "    \n",
        "    accuracy = 0.\n",
        "    total_num_correct = 0\n",
        "    total_num_pred = 0\n",
        "        \n",
        "    for batch in generator: \n",
        "        \n",
        "        # Retrieve the inputs from the batch\n",
        "        inputs = batch[0]\n",
        "        \n",
        "        # Retrieve the targets (actual labels) from the batch\n",
        "        targets = batch[1]\n",
        "        \n",
        "        # Retrieve the example weight.\n",
        "        example_weight = batch[2]\n",
        "\n",
        "        # Make predictions using the inputs            \n",
        "        pred = model(inputs)\n",
        "        \n",
        "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
        "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred,targets,example_weight)\n",
        "                \n",
        "        # Update the total number of correct predictions\n",
        "        # by adding the number of correct predictions from this batch\n",
        "        total_num_correct += batch_num_correct\n",
        "        \n",
        "        # Update the total number of predictions \n",
        "        # by adding the number of predictions made for the batch\n",
        "        total_num_pred += batch_num_pred\n",
        "\n",
        "    # Calculate accuracy over all examples\n",
        "    accuracy = total_num_correct/total_num_pred\n",
        "    \n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "nt1WLxPezGS6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing with your own input"
      ],
      "metadata": {
        "id": "YxGRCaOMzQUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence):\n",
        "    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
        "    \n",
        "    # Batch size 1, add dimension for batch, to work with the model\n",
        "    inputs = inputs[None, :]  \n",
        "    \n",
        "    # predict with the model\n",
        "    preds_probs = model(inputs)\n",
        "    \n",
        "    # Turn probabilities into categories\n",
        "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
        "    \n",
        "    sentiment = \"negative\"\n",
        "    if preds == 1:\n",
        "        sentiment = 'positive'\n",
        "\n",
        "    return preds, sentiment\n"
      ],
      "metadata": {
        "id": "gzUeE0pVzJuo"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try a positive sentence\n",
        "sentence = \"It's such a nice day, I think I'll be taking Sid to Ramsgate for lunch and then to the beach maybe.\"\n",
        "tmp_pred, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
        "\n",
        "print()\n",
        "# try a negative sentence\n",
        "sentence = \"I hated my day, it was the worst, I'm so sad.\"\n",
        "tmp_pred, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4EahTs0zRMw",
        "outputId": "5bc39dcc-bf41-4872-a28b-21bb24e6eac2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the sentence \n",
            "***\n",
            "\"It's such a nice day, I think I'll be taking Sid to Ramsgate for lunch and then to the beach maybe.\"\n",
            "***\n",
            "is positive.\n",
            "\n",
            "The sentiment of the sentence \n",
            "***\n",
            "\"I hated my day, it was the worst, I'm so sad.\"\n",
            "***\n",
            "is negative.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embeddings"
      ],
      "metadata": {
        "id": "ZL54AVlPzYwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.weights[0]"
      ],
      "metadata": {
        "id": "DrMMkXKYzUg4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFfN-FLYzaen",
        "outputId": "3d9673e8-0f8c-4f56-c799-3136a7db1f44"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9088, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA #Import PCA from scikit-learn\n",
        "pca = PCA(n_components=2) #PCA with two dimensions\n",
        "\n",
        "emb_2dim = pca.fit_transform(embeddings) #Dimensionality reduction of the word embeddings"
      ],
      "metadata": {
        "id": "snYFk6ctzcac"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Selection of negative and positive words\n",
        "neg_words = ['worst', 'bad', 'hurt', 'sad', 'hate']\n",
        "pos_words = ['best', 'good', 'nice', 'better', 'love']\n",
        "\n",
        "#Index of each selected word\n",
        "neg_n = [Vocab[w] for w in neg_words]\n",
        "pos_n = [Vocab[w] for w in pos_words]\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "#Scatter plot for negative words\n",
        "plt.scatter(emb_2dim[neg_n][:,0],emb_2dim[neg_n][:,1], color = 'r')\n",
        "for i, txt in enumerate(neg_words): \n",
        "    plt.annotate(txt, (emb_2dim[neg_n][i,0],emb_2dim[neg_n][i,1]))\n",
        "\n",
        "#Scatter plot for positive words\n",
        "plt.scatter(emb_2dim[pos_n][:,0],emb_2dim[pos_n][:,1], color = 'g')\n",
        "for i, txt in enumerate(pos_words): \n",
        "    plt.annotate(txt,(emb_2dim[pos_n][i,0],emb_2dim[pos_n][i,1]))\n",
        "\n",
        "plt.title('Word embeddings in 2d')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "slBhP2fGzdwj",
        "outputId": "a4a355bf-f088-467f-b0f3-b067344582de"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0CiOBxOfzfRN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}